= Binary Document Format
Alex Good <alex@memoryandthought.me>; Andrew Jeffery <andrewjeffery97@gmail.com>
:descriptions: A specification of the Automerge storage format
:revremark: draft
:toc:
:toclevels: 4
:stylesheet: asciidoctor.css

== Introduction

Automerge is a library that allows people to collaboratively work together
without a central co-ordination or a reliable connection.  It is a specific
implementation of of a conflict-free replicated data type (or
https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type[CRDT]).

This document describes the storage format used when serializing Automerge
documents and changes for storage or transfer.

We strongly encourage people to use a library based on the reference
implementation https://github.com/Automerge/Automerge-rs[Automerge-rs] (which is
available as a
https://github.com/Automerge/Automerge-rs/tree/main/rust/Automerge-c[C shared
library] or a
https://github.com/Automerge/Automerge-rs/tree/main/rust/Automerge-wasm[WebAssembly module] for ease of integration). That said, this document should let
you get started building your own, or at least understanding how Automerge
works.

The storage format is designed for compactness and speed of parsing. Automerge 
stores the full history of changes to the document: this is a large amount of
data but in practice it is very repetitive and amenable to compression.

In addition to parsing the storage format, an Automerge library must resolve
conflicts between concurrent operations in a consistent way. This document does
not yet discuss how to do that (pull requests welcome :D), but the reference
implementation should serve as a guide.

== Terminology and Conventions

The key words "`MUST`", "`MUST NOT`", "`REQUIRED`", "`SHALL`", "`SHALL NOT`",
"`SHOULD`", "`SHOULD NOT`", "`RECOMMENDED`", "`NOT RECOMMENDED`", "`MAY`", and
"`OPTIONAL`" in this document are to be interpreted as described in <<RFC2119>>
and <<RFC8174>> when, and only when, they appear in all capitals, as
shown here.


== Concepts

=== Document

An Automerge document is a collaboratively editable JSON-like structure. The
serialized form of a document contains complete history of changes and
operations that collaborators have applied.

Automerge documents have a root that is a map from string keys to arbitrary
<<Value,values>>.

=== Change

A change is a group of <<Operation,operations>> that modify a <<Document,document>>,
analagous to a "commit" in a version control system like git.

Each change is made by an <<Actor,actor>>, and has a (possibly empty) set of 
predecessor changes. Changes have an optional wall-clock timestamp, to keep
track of when a change was committed, and an optional message to describe
meaningful changes.

A change is identified by its <<Change Hash,change hash>> which is the
<<SHA256>> hash of the binary representation of the change.

=== Actor

An actor makes applies a linear sequence of <<Operations,operations>> to a <<Document,document>>
and commits them in 1 or more <<Change,changes>>. Each actor has an actor ID that
uniquely identifies it. An actor ID is an arbitrary sequence of bytes, which
should be generated in a way that will not collide with other actors (the reference
library generates 256-bit random identifiers).

There is a small amount of per-actor overhead, so if you have one process that
edits a document several times, it is preferrable to re-use the same actor
ID for each operation.

=== Operation

An operation is an individual mutation made by an <<Actor,actor>> to a <<Document,document>>.
For example, setting a key to a value or inserting a character into some text.

An operation has an <<Action,action>> which identifies what it does, and various fields
depending on which action. For example an `"inc"` operation has to identify which
counter to increment and by how much.

Each operation is identified by an operation ID. An operation ID is a pair
of (<<Actor,actor ID>>, counter), where the counter is a unique always-incrementing
value per actor.

=== Object

An object represents a collaboratively editable value in an Automerge document. There are three kinds
of object:

* `map` which maps string keys to arbitrary values,
* `list` which is an ordered list of arbitrary values
* `text` which is a collaboratively editable string.

Each object is created by an operation with an `action` of `"makeMap"`,
`"makeList"` or `"makeText"`, and is identified by its object ID. The object ID
is the <<Operation,operation ID>> of the operation that created the object.

Each document has a root `map` which is identified by the object ID `NULL`.

=== Value

Automerge documents are dynamically typed, and can contain any of the following kinds of value:

* `map`, `list`, `text` – the collaboratively editable <<Object,Objects>>
* `null` - an typed null
* `bool` - either `true` or `false`
* `float` - a 64-bit IEEE754 float
* `int` - a 64-bit signed int
* `uint` – a 64-bit unsigned int
* `string` - a utf-8 encoding string (possibly contianing U+0000)
* `bytes` - an arbitrary sequence of bytes
* `timestamp` - a 64-bit signed integer milliseconds since the https://en.wikipedia.org/wiki/Unix_time[unix epoch]
* `counter` - a 64-bit signed intenger that collaborators increment or decrement (instead of overwrite)

== File structure

An Automerge file consists of one or more length delimited chunks.
Implementations must attempt to read chunks until the end of the file.

[#chunk-containers]
=== Chunks

[bytefield, target="chunk-container"]
....
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(def row-height 120)
(draw-column-headers)
(draw-box "magic" {:span 4})
(draw-box "checksum" {:span 4})
(draw-box (text "block type" :vertical))
(draw-box (text "chunk length" :vertical) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-gap "chunk contents")
(draw-bottom)
....

|===
| Field                   | Byte Length         | Description

| Magic Bytes             | 4                   | The sequence `[0x85, 0x6f, 0x4a, 0x83]`
| <<Checksum>>            | 4                   | Validates the integrity of the chunk
| <<Chunk Type>>          | 1                   | The type of this chunk
| Chunk length            | Variable (64-bit <<uLEB>>) | The length of the following chunk bytes
| Chunk contents          | Variable        | The actual bytes for the chunk
|===

If the first four bytes are not exactly the magic bytes implementations MUST abort.

==== Chunk Type
A chunk type is either:

|===
| Value | Type | Description
| `0x00` | <<Document Chunk>> | Contains a graph of related changes
| `0x01` | <<Change Chunk>> | Contains a single change and its operations
| `0x02` | <<Compressed Chunk>> | Either a Document Chunk or Change Chunk that has been DEFLATE compressed
|===

[#document-chunks]
==== Document Chunk

Documents are binary representations of any number of changes and operations
made by any number of actors. The format is heavily inspired by columnular
databases to optimize for efficient compression and fast parsing.

Most fields are of arbitrary length, so parsing the document must proceed in
order; in particular it is not possible to know the length of the column fields
until the column metadata has been parsed.

|===
| Field                                       | Type            | Description                                       

| Actors                                      | <<Array of Actor IDs>>        | The actor IDs in sorted order                     
| Heads                                       | <<Array of Change Hashes>>            | The head hashes of the hash graph in sorted order 
| Change columns metadata                     | <<Array of Column Metadata>>  |
| Operation columns metadata                  | <<Array of Column Metadata>>  | 
| Change columns                              | <<Column Data>>      | The actual bytes for the change columns
| Operation columns                           | <<Column Data>>      | The actual bytes for the operation columns
| Head indexes                                | <<Array of Head Indexes>>     | The indices of the heads in the changes
|===

WARNING: Should we allow for Extra bytes in documents?

A single document contains many changes. Change metadata is encoded separately
to operation data in a column oriented format using the change column metadata
and change bytes above, whilst the operations are encoded using the operations
column metadata and operations bytes. The process of decoding these consists of
first reading all the operation data, then the change metadata using the
procedures outlined in <<column-encodings, column encodings>>, then matching up
operations with their change metadata to construct the reference document.


[#change-chunks]
==== Change Chunk

The fields in a change chunk, in order, are:

|===
| Field | Type | Description

| Dependencies | <<Array of Change Hashes>> | The hashes of dependencies for this change
| Actor length | 64-bit <<uLEB>> | The length of the actor ID
| Actor | bytes | The <<Actor,actor ID>>
| Sequence number | 64-bit <<uLEB>> | The sequence number
| Start op | 64-bit <<uLEB>> | The counter of the first op in this change 
| Time | 64-bit <<uLEB>> | The time this change was created in milliseconds since the unix epoch
| Message length | 64-bit <<uLEB>> | The length of the message in bytes
| Message | UTF-8 encoded string | The message associated with this change
| Other actors | <<Array of Actor IDs>> | Other actor IDs in this change
| Operation columns metadata | <<Array of Column Metadata>> | The metadata for the column oriented operation encoding 
| Operation columns | <<Column Data>> | The actual bytes for the operation columns
| Extra bytes | bytes | All data remaining in the chunk
|===

For forward compatibility the extra data must be retained when writing the
change back to storage.

[#compressed-chunks]
==== Compressed Chunk

Compressed chunks must be decompressed using <<DEFLATE>>. The decompressed chunk
should be either a <<Document Chunk>> or <<Change Chunk>>. Implementations
SHOULD raise an error if the contents of a compressed chunk is another
compressed chunk.

==== Checksum

The checksum is the first four bytes of the <<SHA256>> hash of the concatenation of the chunk length
and chunk contents fields. The checksum calculated from this hash is the first
four bytes of the hash. Implementations MUST abort reading if the checksum does
not match.

==== Change Hash

A change hash is the <<SHA256>> hash of the concatenation of the chunk length
and chunk contents fields of a change represented as a <<Change Chunk>>.

=== Simple types

==== uLEB

uLEB is an unsigned https://en.wikipedia.org/wiki/LEB128[little endian base 128] value.
This is a variable length encoding used throughout.

To encode a uLEB, represent the number in binary and pad it with leading zeros
so that it has a length which is a multiple of 7. Take each group of 7 bytes from
least-significant to most-significant and output them in bytes - the first bit
of every byte is 1 except for the last byte which is 0.

* Unsigned ints 0 - 127 are stored as one byte: `0b00000000 - 0b01111111`
* Unsigned ints 128 - 16383 are stored as two bytes: `0b10000000 0b00000001 - 0b11111111 0b01111111`
etc.

To decode a uLEB, read bytes up to and including the first byte with a 0 as the
first bit.  Take the latter 7 bits from each byte (the last byte contains the
most significant bits, so you need to concatenate them in the opposite order to
which the bytes are represented on disk).

Although uLEB encoding can support numbers of arbitrary bitsize, fields in
Automerge are size limited to 32, 53 or 64 bits. Implementations should fail to
parse documents that contain overly large encoded integers in fields.

Implementations must not generate overly long encodings, and should reject
documents with overly long encodings. For example using the decoding rules above
the bytes `0b10000000 0b00000000` would be decoded as 0; but this is overly
long: 0 can be represented in just one byte as `0b00000000`.

==== LEB

LEB is a signed variant https://en.wikipedia.org/wiki/LEB128[little endian base 128] value

To encode a uLEB, represent the number in twos complement, and sign-extend it so
that it has a length which is a multiple of seven. If the number is negative the padding will
be of 1-bits and if the number is positive the padding will be 0-bits.

* 0 is represented as one byte: `0b0000000`
* Ints from 1 to 63 are represented as one byte: `0b00000001 - 0b00111111`
* Ints from -1 to -64 are represented as one byte: `0b01111111 - 0b010000000`
* Ints from 64 to 8191 are represented as two bytes: `0b11000000 0b00000000 - `0b11111111 0b00111111`
* Ints from -65 to -8192 are represented as two bytes: `0b10111111 0b01111111 - 0b10000000 0b01000000`
etc.

To decode an LEB, read bytes up to and including the first byte with a 0 as the
first bit.  Take the latter 7 bits from each byte (the last byte contains the
most signfiicant bits, so you need to concatenate them in the opposite order to
which the bytes are represented on disk). If the first bit of your number is 1
(from the second bit of the last byte in encoded form) then you have a negative
number and you can take twos complement to get to its absolute value; otherwise
you have a positive number (or 0).

Implementations must not generate overly long encodings, and should reject
documents with overly long encodings.  For example the decoding rules above the
bytes `0b11111111 0b01111111` would be decoded as -1; but this is overly long: -1 can be represented as just one byte `0b01000000`.

==== Action

The actions of the reference data model are encoded in the storage format as a
byte as follows:

|===
| Byte | Action      | Description

| 0x00 | `makeMap`   | Creates a new map object
| 0x01 | `set`       | Sets a key of a map, overwrites an item in a list, inserts an item in a list, or edits text
| 0x02 | `makeList`  | Creates a new list object
| 0x03 | `del`       | Unsets a key of a map, or removes an item from a list (reducing its length)
| 0x04 | `makeText`  | Creates a new text object
| 0x05 | `inc`       | Increments a counter stored in a map or a list
|===

==== Column Specification

Column specifications are a 32-bit integer <<uLEB>> encoded which should be
interpreted as a bitfield like so:

[bytefield,target="column-id-layout"]
....
(def boxes-per-row 32)
(def row-height 100)
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(draw-column-headers {:labels (map str (reverse (take 32 (iterate inc 1))))})
(draw-box "ID" {:span 28})
(draw-box (text "DEFLATE" :vertical) {:span 1})
(draw-box "type" {:span 3})
....

* The least significant three bits encode the column type
* The 4th least significant bit is `1` if the column is <<DEFLATE>> compressed and
  `0` otherwise
* The remaining bits are the column ID

If the deflate bit is set then the column data must first be decompressed using
DEFLATE before proceeding with decoding the values.

The ID defines the purpose of the column for either <<Change Columns>> or
<<Operation Columns>>, and implementations must preserve columns that they do
not understand.

The column type specifies how the data in the column is encoded. The possible
types are:

[#column-types-table]
|===
| Value | Description

| 0 | <<Group Column>>
| 1 | <<Actor Column>>
| 2 | <<uLEB Column>>
| 3 | <<Delta Column>>
| 4 | <<Boolean Column>>
| 5 | <<String Column>>
| 6 | <<Value Metadata Column>>
| 7 | <<Value Column>>
|===

=== Compound types

==== Array of Actor IDs

The actor ID array consists of a 64-bit <<uLEB>> giving the count of actor ids, followed by 
each actor ID as a length-prefixed byte array.

For example an array consisting of the single actor ID `[0xab, 0xcd, 0xef]`
would be encoded as: `0x01 0x03 0xab 0xcd 0xef`.

Implementations must store actor ids lexicographically, and should error when
reading a document with actor ids in the wrong order.

==== Array of Change Hashes

The heads array consists of a 64-bit <<uLEB>> giving the count of heads, followed by each 32-byte head.

For example an array consisting of the heads `f986a4318d1f1cc0e2e10e421e7a9a4cd0b70a89dae98bc1d76d789c2bf7904c` and `4355a46b19d348dc2f57c046f8ef63d4538ebb936000f3c9ee954a27460dd865` would be represented as `0x02 0xf9 0x86 ..{28 bytes elided).. 0x90 0x4c 0x43 0x55 ..{28 bytes elided}.. 0xd8 0x65`

==== Array of Head Indexes

The head indexes array consists of one 64-bit <<uLEB>> per head which identifies
the change that the head references. They are stored in the same order
as the heads in the <<Array of Change Hashes>> at the start of the <<Document Chunk>>

For example if your document contains two heads and two changes, the array might
consist of a 0 and a 1, encoded  as `0x00 0x01`

WARNING: What is the point of this field?

==== Array of Column Metadata

The column metadata array consists of a 64-bit <<uLEB>> N giving the number of columns, followed by N pairs describing each columns

|===
| Field | Description

| Column Specification | a <<uLEB>> encoded <<Column Specification>>
| Column Length | 64-bit <<uLEB>> of the length (in bytes) of the column data
block 
|===

The specifications must be unique and sorted. Because columns may optionally be compressed,
the sort order is defined by setting the deflate bit to 0. Implementations must not include
both an uncompressed and a compressed column with the same ID and type.

A column may have length 0 if every value in the column is null.

=== Column Data

Columns are stored one after the other with no separators or length indicators.
The column are stored in order they appear in the <<Array of Column Metadata>>
and they can be decoded according to the <<Column Specification>>.

All columns must have the same number of items, though as they are compressed
differently they may have vastly different byte counts.

For future compatibility it is important that programs which edit Automerge
documents maintain all columns, even those that they don't understand the
meaning of. When new data is added a null should be added to any unknown column
following the encoding rules of its <<Column Specification>>.

==== Change Columns

The currently defined columns for changes in a <<Document Chunk>> are:

|===
| Name | Specification | Type | Description

| Change actor | 1 | <<Actor Column>>  |
| Sequence number | 3 | <<Delta Column>> | The sequence number for each change
| maxOp | 19 | <<Delta Column>> | The largest counter that occurs in each change
| time | 35 | <<Delta Column>> | The time at which each change was made
| message | 53 | <<String Column>> | The commit messages for each change
| dependencies group | 64 | <<Group Column>> |
| dependencies index | 67 | <<Delta Column>> |
| value metadata | 86 | <<Value Metadata Column>> | The metadata for any extra data for this change
| value | 87 | <<Value Column>> | The data for any extra data for this change
|===

Each row in the column oriented change metadata therfore can be written as:

|===
| Field | Type | Mapping

| Actor | positive integer | lookup_actor(change actor)
| Seq | positive integer | sequence number
| maxOp | positive integer | maxOp
| time | positive integer | time
| message | utf-8 | message
| deps | list of integers | read dependencies group and dependencies
index columns
| extra | primitive value | read the value metadata and value raw columns
|===

The `deps` field refers to the index of the changes this change depends on in
the change metadata rows. Implementations MUST abort if `deps` references an
index which is out of bounds.

For a given actor the `seq` field of changes must strictly increase by `1`.
Implementations MUST abort if there are missing changes for a given actor ID.

The `maxOp` field of the change refers to the largest counter component of an
operation ID in the set of operations in this change. For a given actor ID this
must always increase. Implementations MUST abort if the `maxOp` of a change is
not larger than all the `maxOp` of changes from that actor with smaller `seq`.


==== Operation Columns

The currently defined columns for operations in a <<Document Chunk>> or a <<Change Chunk>> are:

|===
| Field | Specification | Type | Description
 
| Object actor | 1 | <<Actor Column>> | actor index of object ID each operation targets
| Object counter | 2 | <<uLEB Column>> | counter of the object ID each operation targets
| Key actor | 17 | <<Actor Column>> | actor of the operation ID of the key of each operation
| Key counter | 19 | <<Delta Column>> | counter of the operation ID of the key of each
  operation
| Key string | 21 | <<String Column>> | The string key each operation targets
| actor | 33 | <<Actor Column>> | The actor of each operations ID
| counter | 35 | <<Delta Column>> | The counter of each operations ID
| insert | 52 | <<Boolean Column>> | Whether or not this is an insert operation
| action | 66 | <<uLEB Column>> | The <<Action>> of each operation
| value metadata | 86 | <<Value Metadata Column>> | The metadata for the value of this operation
| value | 87 | <<Value Column>> | The value of this operation
| successor group | 128 | <<Group Column>> | The group for the successors of this operation
| successor actor | 129 | <<Actor Column>> | The actor of each successor operation ID of this operation
| successor counter | 131 | <<Delta Column>> | The counter of each successor operation ID of this operation
|===

WARNING: The javascript implementation includes a `child` column, is this
required?

We determine the key that the operation refers to thusly:

* If the key string is not null then this is the key of the operation
* Otherwise we use the pair (lookup_actor(key actor), key counter) as the key of the operation
* If key string is null and any of key actor or key counter are null
  implementations MUST abort

Using this procedure we can write the operations as:

|===
| Field | Type | Mapping to columns

| Object | Operation ID | (lookup_actor(object actor), object counter)
| Key | either string or operation ID | The value determined above
| Id | Operation ID | (lookup_actor(actor), counter)
| Insert | boolean | insert
| <<Action>> | action | action
| Value | primitive value | value metadata and value columns
| Successors | list of operation ID | (lookup_actor(actor), counter) for actor,
and counter in the success group column
|===

=== Column Types


==== Run Length Encoding

Many columns use run length encoding to compress repeated values. Such columns are
encoded as repeated pairs of the form `(length, value)`.

A "run" in an RLE columns is encoded as pairs of the form `(length,value)`.
`length` is a signed <<LEB>> encoding of the length of the run. the interpretation
of `value` depends on `length`.

* If `length` is positive, then `value` is a single instance of the value which
  occurs `length` times.
* If `length` is 0 then this pair represents a `null` value and `value` is the
  uLEB encoding of the number of times `null` occurs
* If `length` is negative then `value` is a literal run and the absolute value
  of `length` is the number of items in the literal run. That is to say, there
  is no compression.

For example if you were trying to compress the array of uLEBs `[0,0,0,null,null,1,2,3]`
you might encode that as `0x03 0x00 0x00 0x02 0x7d 0x01 0x02 0x03`

[#actor-index-columns]
==== Actor Column

An actor column uses <<Run Length Encoding>> to compress a list of <<uLEB>>s
that represent an index into an array of actor ids.

In a <<Document Chunk>> the index is the position of the actor id in the <<Array of Actor IDs>>.

In a <<Change Chunk>> index 0 represents the actor id of the change, and index 1+ are given to the
other actor ids in the order they appear.

[#group-columns]
==== Group Column

A group column specifies a composite, collection-valued column. Column
specifications following the group column specification in the metadata block
which have the same ID as the group column specification should be read
together, these are the "grouped columns". The group column data consists of
<<rle-columns, run length encoded integers>>, the value for each row determines
how many values should be read from each of the grouped columns. Implementations
MUST abort if they cannot read this number of values from each of the grouped
columns.

An example of this is the `pred` column in the change encoding. The portion of
the metadata block containing the pred column specification is encoded thusly

[svgbob, target="group-example"]
....
.-----+------------+-----+------------+-----+-----------.
| 112 | <data len> | 113 | <data len> | 115 | <data len>|
| ...                                                   |
`-------------------------------------------------------'
....

* `112` is `(7 << 4)`, thus the type is `0` which means this is a group column.
  With ID `7`
* `113` is `(7 << 4) | 1` so the type is `1` which is "actor" and the column
  id is `7`
* `115` is `(7 << 4) | 3` so the type is `3` which is "delta int" and the column
  ID is `7`

To read values from this column then we first decode the value of the group
column, then we decode this number of values from each of the grouped columns
and the value for the row becomes the list of lists of resulting values. In this
case if we read `n` from the group column then the row value would be `[[actor1,
counter1], [actor2, counter2], ..., [actor_n, counter_n]]`

Note that it is not possible for two columns in a group to have the same type as
it would not be possible to have a deterministic ordering for the column
specifications. Implementations MUST abort if they encounter two column
specifications with the same type and column ID.

Implementations MUST abort if they encounter multiple group column
specifications with the same ID.

Group column specifications must be followed by at least one column
specification with the same column ID. Implementations MUST abort if a group
column specification without a following column specification of the same ID is
encountered.


==== uLEB Column

A uLEB column uses <<Run Length Encoding>> to compress a list of 64-bit numbers
encoded as <<uLEB>>s.

==== Delta Column

A delta column uses <<Run Length Encoding>> to compress a list of 64-bit numbers
encoded as signed <<LEB>>s.

The first number in the list is the starting value of the column, and subsequent
numbers represent the difference between the next value and the current value.

For example, if you wanted to encode the list [3,4,5,6,9,7,8] you would first
delta encode to get [3,1,1,1,3,-2,1], and then run-length encode to
`0x7f 0x03 0x03 0x01 0x7d 0x03 0x7e 0x01`.

WARNING: How should applications handle a decoded delta value which takes the
absolute value below zero?

==== Boolean Column

This encoding is only available for columns containing booleans. The column
contains sequences of uLEB integers which represent alternating sequences of
`false/true`. The initial value of the column is always `false`

For example, the sequence `[0,2,3]` would be `[true, true, false, false,
false]`.


[#raw-value-columns]
==== Value Metadata Column

The Value metadata column is always paired with a <<Value column>> with the same
ID. The metadata column is a <<Run Length Encoding>> compressed list of 64-bit
<<LEB>>s


==== Value Column

Raw value fields are encoded as two column specifications. The first has type
`6`, indicating that it is raw value metadata and the second has type `7`,
indicating that it contains raw values. The two columns have the same ID. 

Note that raw value columns which do not contain values may be omitted. If
implementations encounter a lone value metadata column they must assume that it
is accompanied by an empty raw value column.

Implementations must abort if they encounter  a raw value column not preceeded
by a metadata column with the same id. implementations must also abort if they
encounter more than one metadata column with the same column id, or more than
one raw value column with the same id.

These two colums are intepreted together. The metadata column contains RLE
compressed LEB integers. These integers are laid out like so

[bytefield,target="raw-value-metadata-layout"]
....
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(draw-column-headers {:labels ["64", "63", "62", "61" ,"60", "59","58","57","56","...","6","5","4","3","2","1"] } )
(draw-box "length" {:span 12})
(draw-box "type" {:span 4})
....

* The lower four bits encode the type of the value
* The higher bits encode the length of the value

The type code may be 

|===
| Value | Type 

| 0 | Null
| 1 | False
| 2 | True
| 3 | uLEB 
| 4 | LEB
| 5 | IEEE754 float
| 6 | UTF8 bytes
| 7 | Bytes
| 8 | Counter
| 9 | Timestamp
|===

If the type tag is none of these values it may be a value produced by a future
version of Automerge. In this case implementations MUST read and store the type
code and raw bytes when reading and write them back in same position when
writing.

The interpretation of the value column depends on the type code. 

* For `0,1,2` (`null`, `false`, `true`) no value is stored in the raw value
  column
* For all other column types the length bits specify the number of bits which
  should be read from the raw value column (which is not compressed in any
  manner) and interpreted as follows:
** `uLEB` and `LEB` as per the LEB128 spec
** IEEE754 floats - as per the spec
** UTF8 bytes should be interpreted as a string. Implementations SHOULD validate
   that the bytes are valid UTF8 and replace any offending characters with
   U+FFFD REPLACEMENT CHARACTER
** Bytes - the data is an arbitrary byte sequence
** Counter, the underlying data is a uLEB encoded integer.
** Timestamp, the underlying data is a uLEB encoded integer.

WARNING: Replacing invalid utf-8 seems like it might be a bad idea? Should check
this. I _think_ it's what the javascript implementation does though.

[#unknown-columns]
=== Unknown columns

When reading the column metadata applications may encounter column
specifications which they are not expecting. These column specifications may be
produced by future versions of the application. If an implementation encounters
an unknown column whilst reading data it MUST retain this data when writing that
data back to storage.

This is possible because every column type has some concept of a null value.
When inserting new rows into a collection of rows stored in the columnar storage
format application MUST write a null value into columns which they do not
recognise for the new rows they are inserting.

WARNING: What should the null value be for boolean or delta columns?

[#document-actor-lookup]
==== Actor lookup

Actors in the document encoding are encoded in lexicographic order in the actors
array at the start of the document. Actor indexes throughout the document refer
to the index into this array. We use the syntax `lookup_actor(actor_index)` to
refer to this procedure.


==== Order of operations

Operations are grouped by the object that they manipulate. Objects are then
sorted by their IDs. Thus operations are ordered using the following procedure:

WARNING: Is this required? If so should implementations abort if the operations
are not inthis order?

* First sort by object ID, such that any operations for the same object are
  consecutive. The null objectId (i.e. the root object) is sorted before all
  non-null objectIds. Non-null objectIds are sorted by <<lamport-timestamp,
  Lamport timestamp>>.
* For each object:
** if the object is a map, sort the operations within that object
   lexicographically by key, so that all operations for the same key are
   consecutive. This sort order MUST be based on the UTF-8 byte sequence of the
   key. 
** If the object is a list or text, sort the operations within that object by the
   operation ID of the element they target. This is determined as follows:
*** For insert operations the target element is the operation ID of the
    inserting operation
*** For `set` or `delete` operations the target is the operation ID in the `key`
    field
* Among the operations for the same key (for maps) or the same list element (for
  lists/text), sort the operations by their opId, using <<lamport-timestamp,
  lamport timestamp>> ordering. For list elements, note that the operation that
  inserted the operation will always have an opId that is lower than the opId of
  any operations that updates or deletes that list element, and therefore the
  insertion operation will always be the first operation for a given list
  element.


WARNING: the JavaScript implementation currently does not do this sorting
correctly, since it sorts keys by JavaScript string comparison, which differs
from UTF-8 lexicographic ordering for characters beyond the basic multilingual
plane.

===== Successors and omitting deletes

The document storage format does not encode a predecessors field. Instead this
information is encoded in the `successors` field. This can be used to
reconstruct the predecessors field from the reference data model.

Delete operations do not carry any information other than the object ID and key
they are deleting. As such they are encoded in the document by appending the
operation ID of the delete operation to the successors of the operation creating
the data to be deleted.

Implementations MUST abort if they encounter explicitly encoded delete
operations in a document chunk.

[#document-change-metadata]
==== Change Metadata

The columns in the change metadata are at least the following:

|===
| Name | Specification | Type | Description

| Change actor | 1 | Actor  |
| Sequence number | 3 | Delta compressed uLEB |
| maxOp | 19 | Delta compressed uLEB | The largest counter that occurs in this
change
| time | 35 | Delta compressed uLEB |
| message | 53 | RLE Compressed UTF-8 |
| dependencies group | 64 | Group |
| dependencies index | 67 | Delta compressed uLEB |
| value metadata | 86 | Value metadata |
| value | 87 | Value raw |
|===

Any unknown columns MUST be preserved when decoding and written back out when
encoding as per <<unknown-columns, unknown columns>>.

Each row in the column oriented change metadata therfore can be written as:

|===
| Field | Type | Mapping

| Actor | positive integer | lookup_actor(change actor)
| Seq | positive integer | sequence number
| maxOp | positive integer | maxOp
| time | positive integer | time
| message | utf-8 | message
| deps | list of integers | read dependencies group and dependencies
index columns
| extra | primitive value | read the value metadata and value raw columns
|===

The `deps` field refers to the index of the changes this change depends on in
the change metadata rows. Implementations MUST abort if `deps` references an
index which is out of bounds.

For a given actor the `seq` field of changes must strictly increase by `1`.
Implementations MUST abort if there are missing changes for a given actor ID.

The `maxOp` field of the change refers to the largest counter component of an
operation ID in the set of operations in this change. For a given actor ID this
must always increase. Implementations MUST abort if the `maxOp` of a change is
not larger than all the `maxOp` of changes from that actor with smaller `seq`.

==== Mapping to the reference data model

Operations in the document format are not stored in the order they were
generated, as they are in the change data model. Furthermore, oeprations in the
document format have a `successor` rather than `predecessor` field. The
following procedure specifies how to map from document operations to the change
operations. "document operation" refers to the data structure derived at the end
of <<document-operations, document operations>> and "document change" refers to
the data structure dervied at the end of <<document-change-metadata>>.

First expand operations:

* Add an empty predecessor list to every document operation
* For each operation in the document operation rows
** For each operation ID in the successors list of the document operation lookup
   the target operation in the document operations:
*** If an operation is found add the current operation ID to the
    target operations predecessor list
*** If no operation is found then insert a new delete operation into the
    document with its ID set to the target operation ID, the object and key
    set to the same value as the current operation, and the predecessor set to
    the current operation.

Second, match up changes:

For each document operation

* Sort all the changes for the same actor as the operation ID by ascending
  `maxOp`
* Add the document operation to the first change which has `maxOp >= counter`
  where `counter` is the counter component of the operation ID.

Implementations MUST abort if no matching change is found

For each change sort the operations within the change by
<<lamport-timestamp>> of the operation ID.

===== Hash verification

The dependencies in the document model are expressed as integer offsets. But in
the reference data model dependencies are expressed as a hash of the ancestor
changes. To map to the hash based representation perform a topological traversal
of the dependency graph and for each change serialize the change as a <<Change
Chunk>> then calculate the hash of the change as in the <<Change Hash>>, then
for every change replace the index of the current change with the calculated
hash.

Once this procedure is complete take the heads of the depedency graph and
compare their hashes with the head hashes field in the document chunk. If the
hashes don't match implementations MUST abort.

[#change-operation-columns]
==== Change operation columns

The column specifications in the operation metadata must include the following
(note that the column types are redundant as they are included in the
specification but we elaborate them for clarity):


|===
| Field | Specification | Type | Description

| object actor |1   | Actor Index | The actor of the ops object ID
| object counter  |2   | RLE compressed uLEB | The counter of the ops object ID
| key actor |17  | Actor | The (optional) actor of the ops key 
| key counter |19  | Delta Compressed uLEB | The (optional) counter of the ops key 
| key string |21  | RLE Compressed UTF-8 | The (optional) string of the ops key
| ID actor |33  | Actor index | The actor of the ops op ID
| ID counter |35  | Delta compressed uLEB | The counter of the ops op ID
| insert |52  | Boolean | Whether or not this is an insert operation
| action index |66  | RLE compressed uLEB | The <<action-array, action index>> for the op
| value metadata |86  | Value meta | The value metadata for the op
| value raw |87  | Value raw | The raw value for the op
| pred group |112 | Group | The <<group-columns, group column>> for the
predecessors of this op
| pred actor index |113 | Actor | The actor component of the predecessors
| pred counter |115 | RLE Compressed uLEB | The counter component of the predecessors
|===

WARNING: The javascript implementation includes a `child` actor ID here. It
doesn't seem to be needed though, is it obsolete?

Reading implementations MUST abort if any of these column specifications are not
present.

There may be additional columns present, implementations MUST read these columns
when translating to the reference data model as per <<unknown-columns, unknown
columns>>.

===== Compressed columns

Compressed columns are not permitted in change chunks. Implementations MUST
abort if they encounter a column specification with the deflate bit set.

[#change-actor-lookup]
==== Actor ID lookup

All actor columns resolve to integers. These integers are offsets into the
concatenation `[change actor ID] + other actor IDs` from the change metadata.
Implementations MUST abort if an actor index is read which is not present in
this concatenation.

==== Mapping to the reference data model

We lookup actors using the notation `lookup_actor(actor Index)` which refers to
the process specified in <<change-actor-lookup, actor ID lookup>>.

We determine the key that a row refers to thusly:

* If the key string column is not null then this is the key of the operation
* Otherwise we use the pair (lookup_actor(key actor), key counter) as the key of the operation
* If key string is null and any of key actor or key counter are null
  implementations MUST abort

We can then map the columns to the reference data model as follows:

|===
| Field | Mapping from column

| Object ID | `(lookup_actor(object actor), object counter)`
| Op ID | `(lookup_actor(ID actor), ID counter)`
| Key | The key determined above
| Action | The action from the <<action-array>> corresponding to the action
index
| Value | The value read from the value metadata and value raw columns
| Pred | `(lookup_actor(pred_actor), pred_counter)` for pred_actor and
pred_counter in the predecessors group column
|===

For each unknown column in the column metadata implementations MUST add the
value of that unknown column to the reference operation. Implementations MUST
store the column specifications as well as the values so that the unknown values
can be written back out when mapping from the reference operation back to the
change chunk.

The reference change then becomes:

|===
| Field | Change metadata field

| Actor ID | lookup_actor(change actor)
| Seq | Change sequence number
| Message | Change message
| Dependencies | Change dependencies
| Operations | The operations mapped above
| Extra bytes | The extra bytes in the change
|===

[#length-prefixed-actor-ids]
=== Length prefixed actor IDs

Actor IDs are stored in length prefixed form as follows

[svgbob, target="length-prefixed-actor"]
....
.--------------+-------.
| Length: uLEB | Bytes |
`--------------+-------'
....


[bibliography]
== References

* [[[RFC2119]]]: https://datatracker.ietf.org/doc/html/rfc2119
* [[[RFC8174]]]: https://datatracker.ietf.org/doc/html/rfc8174
* [[[DEFLATE]]]: https://datatracker.ietf.org/doc/html/rfc1951
* [[[SHA256]]]: https://datatracker.ietf.org/doc/html/rfc4634

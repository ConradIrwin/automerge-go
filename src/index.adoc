= Binary Document Format
Alex Good <alex@memoryandthought.me>; Andrew Jeffery <andrewjeffery97@gmail.com>
:descriptions: A specification of the automerge storage format
:revremark: draft
:toc:
:stylesheet: asciidoctor.css

== Introduction

Automerge documents are a directed acyclic graph (DAG) of changes. Each change
consists of an actor ID, a list of parent changes, and a list of operations.
We refer to these conceptual structures as the "reference document", "reference
change", and "reference operation". The storage format achieves good compression
by using a specific representation of the reference data structures, this
specification describes how to transform between the compressed and reference
representations.

== Concepts

=== Actor IDs

Actors represent sequential threads of operation in automerge. An actor ID is an
arbitrary sequence of bytes.

=== Operations

Operations are individual mutations of a document which are bundled together in
changes. Some operations carry a payload.

=== Operation IDs

Every operation is identified by the pair (`actor ID`, `counter`). The actor ID
is the ID of the actor who created the operation, counter is an integer which
always increases for each actor.

[#objects-intro]
=== Objects and object IDs

Some data types in an automerge document are composite objects, either maps from
strings to values - the `map` and `table` objects - or sequences - `list` and
`text`. Operations on these objects reference objects by the operation ID of the
operation which creates the object. The null object ID indicates that the
operation target is the "root" object, which is a map.

=== Dependency Graph

Changes in automerge form a DAG expressed by a list of `SHA256` hashes of direct
ancestors stored in the `dependencies` field of the <<change-reference,
change>>.


=== Reference Data Structures

[#change-reference]
==== Change

|===
| Field | Type | Description

| Actor ID | Arbitrary byte sequence | Unique actor ID
| Seq | 64 bit Integer | Sequence number, always increasing per-actor
| Start Op | 64 bit Integer | Sequence number of first operation in the change
| Message | Optional byte sequence | Human readable message describing this
change
| Dependencies | List of 32 byte arrays | List of hashes of parent changes
| Operations | List of operations | The operations in this change
| Extra bytes | Arbitrary byte sequence | Extra data reserved for forward
compatbility reasons
|===

==== Operations

|===
| Field | Type | Description

| Object ID | Operation ID | The ID of the object this operation changes
| Key | String or operation ID | The map property or sequence element within the
object
| Action | Action | The change this operation is making
| Value | Optional primitive value | The payload of this operation (if any)
object,that this operation pertains to
| Pred | List of operation IDs | Previous operations this operation supercedes
|===

The action of an operation can be one of a few different types:

`makeMap`, `makeTable`, `makeList`, `makeText` :: Operations which denote
creation of a new composite object. The ID of the operation becomes the ID of
the resulting object as noted in <<objects-intro,objects>>.
`del` :: Marks the key within the object as deleted
`inc` :: Increments the counter stored at the given object and key
`set` :: Set the value at the given object and key

The `inc` and `set` operations have an associated `value`. For all other
operations `value` is `null`.

[#primitive-values]
==== Primitive Values

|===
| Type | Description

| bytes | Arbitrary sequnce of bytes 
| string | A valid UTF-8 string
| int | 64 bit integer
| float | 64 bit floating point number
| counter | 64 bit positive integer
| timestamp | 64 bit positive integer
| boolean | boolean
| null | the null value
|===

Technically the `counter` and `timestamp` types are not primitive but they are
still treated separately in the data model.

=== Column oriented storage

The storage format achieves good compression by storing operations and changes
in a column oriented manner. That is to say that rather than storing data like
this:

[svgbob, target="column-oriented-intro-rows"]
....
.---------------------------------------------------.
|actor ID | object ID | key | action | value | pred | <- 1st operation
|actor ID | object ID | key | action | value | pred | <- 2nd operation
| ...                                               |
`---------------------------------------------------'
....

It is stored like this:

[svgbob, target="column-oriented-intro-columns"]
....
.---------------+---------------+-----.
| op1 actor ID  | op2 actor ID  | ... |
|---------------+---------------+-----|
| op1 object ID | op2 object ID | ... |
|---------------+---------------+-----|
| op1 key ID    | op2 key ID    | ... |
|---------------+---------------+-----|
| op1 action ID | op2 action ID | ... |
|---------------+---------------+-----|
| op1 value ID  | op2 value ID  | ... |
|---------------+---------------+-----|
| op1 pred ID   | op2 pred ID   | ... |
`---------------+---------------+-----'
        ^               ^
        |               |
 1st operation    2nd operation
....

i.e. The data for each column is stored contiguously, rather than for each row.


== Encoding

=== uLEB and LEB

uLEB is an unsigned https://en.wikipedia.org/wiki/LEB128[little endian base 128] value.
This is a variable length encoding used throughout this document.

LEB is the signed variant.

=== Column Metadata

Data stored in columnar format is made up of two parts, a metadata block and a
data block. The metadata block is length delimited:

|===
| Field | Description

| Num columns | uLEB of the number of columns in the metadata
| Column metadata | The bytes containing the  metadata
|===

The column metadata consists of pairs of the form

|===
| Field | Description

| <<column-specifications, Column Specification>> | a 32 bit integer
| Column data length | uLEB encoding of the length of the data for this column in the data
block 
|===

The data for each column is in the data block in the same position as the
respective column occurs in the metadata block. The column specification encodes
how to interpret the data in the data block.

==== Order of columns

Columns MUST be encoded in ascending <<normalized-column-specifier, normalized
column specification>> order, implementations MUST abort parsing with an error
when reading a document if the columns are not in this order.

The column data MUST be encoded in the data block in the same order as the
column metadata.

[#column-specifications]
==== Column Specifications

Column specifications are a 32 bit field encoded like this (column numbering in
bigendian order):

[bytefield,target="column-id-layout"]
....
(def boxes-per-row 32)
(def row-height 100)
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(draw-column-headers {:labels (map str (reverse (take 32 (iterate inc 1))))})
(draw-box "ID" {:span 28})
(draw-box (text "DEFLATE" :vertical) {:span 1})
(draw-box "type" {:span 3})
....

* The least significant three bits encode the column type
* The 4th least significant bit is `1` if the column is DEFLATE compressed and
  `0` otherwise
* The remaining bits are the unique column ID

Implementations MUST abort with an error if duplicate column specifiers are
detected when parsing.

If the deflate bit is set then the column data must first be decompressed using
DEFLATE before proceeding with decoding the values.

The column type specifies how the data in the column is encoded. The possible
types are:

[#column-types-table]
|===
| Value | Description | Encoding

| 0 | <<group-columns,Group>> | RLE compressed uLEB
| 1 | Actor ID | RLE compressed integer
| 2 | Arbitrary integers | RLE compressed LEB
| 3 | Arbitrary integers | Delta compressed uLEB
| 4 | Arbitrary booleans | Boolean
| 5 | Arbitrary strings | RLE compressed string
| 6 | Raw value metadata | The length and type of items in an accompanying raw value column
| 7 | Raw values | Raw values
|===


[#normalized-column-specifier]
===== Normalized column specifiers

Because columns can be optionally compressed there are two possible encodings of
the same column specifier - one with and one without the compression bit set.
Column specifiers are normalized by setting their 4th least significant bit to
0.

An example algorithm for normalizing columns specifiers is the following
(assuming integers are represented in bigendian order, and `&&` performs bitwise
AND).

[listing]
....
normalizedIds = []
for id in columnIds
    normalizedId = id && 0x8
    normalizedIds.push(normalizedId)
....



[#column-encodings]
=== Column Encodings

[#group-columns]
==== Group

A group column specifies a composite, collection-value column. Column specifiers
following the group column specifier in the metadata block which have the same
ID as the group column specifier should be read together. The group column
data consists of <<rle-columns, run length encoded integers>>, the value for
each row determines how many values should be read from each of the grouped
columns. Implementations MUST abort with an error if they cannot read this
number of values from each of the grouped columns.

An example of this is the `pred` column in the change encoding. The portion of
the metadata block containing the pred column specifier is encoded thusly

[svgbob, target="group-example"]
....
.-----+------------+-----+------------+-----+-----------.
| 112 | <data len> | 113 | <data len> | 115 | <data len>|
| ...                                                   |
`-------------------------------------------------------'
....

* `112` is `(7 << 4)`, thus the type is `0` which means this is a group column.
  With ID `7`
* `113` is `(7 << 4) | 1` so the type is `1` which is "actor" and the column
  id is `7`
* `115` is `(7 << 4) | 3` so the type is `3` which is "delta int" and the column
  ID is `7`

To read values from this column then we first decode the value of the group
column, then we decode this number of values from each of the grouped columns
and the value for the row becomes the list of lists of resulting values. In this
case if we read `n` from the group column then the row value would be `[[actor1,
counter1], [actor2, counter2], ..., [actor_n, counter_n]]`

Note that it is not possible for two columns in a group to have the same type as
it would not be possible to have a deterministic ordering for the column
specifiers. Implementations MUST abort with an error if they encounter two
column specifiers with the same type and column ID.

Implementations MUST abort with an error if they encounter multiple group
column specifiers with the same ID.


[#rle-columns]
==== RLE

Run length encoding of values. The exact type of value depends is specified by
the column type in <<column-types-table, column types>>. A "run" is encoded as
pairs of the form `(length,value)`. `length` is a signed LEB encoding of the
length of the run. the interpretation of `value` depends on `length`.

* If `length` is positive, then `value` is a single instance of the value which
  occurs `length` times.
* If `length` is 0 then this pair represents a `null` value and `value` is the
  uLEB encoding of the number of times `null` occurs
* If `length` is negative then `value` is a literal run and the absolute value
  of `length` is the number of items in the literal run. That is to say, there
  is no compression.


==== Delta

This encoding is only applicable for columns which contain positive integer
datatypes. The encoded data is a sequence of uLEB integers. The value starts as
`0` and each new item is encoded as the difference between the new value and the
current value. This sequence of deltas is then run length encoded as per the run
length encoding section.

For example, the sequence 

|===
|1|2|3|4|5|10|15
|=== 

Would be encoded as 

|===
|1|1|1|1|1|5|5
|===

This sequence is then run length encoded to given

|===
| (5,1) | (2,5) 
|===

==== Boolean

This encoding is only available for columns containing booleans. The column
contains sequences of uLEB integers which represent alternating sequences of
`false/true`. The initial value of the column is always `false`

For example, the sequence `[0,2,3]` would be `[true, true, false, false,
false]`.


== File structure

An automerge file consists of one or more length delimited chunks.
Implementations must attempt to read chunks until the end of the file. There are
three types of chunk, one which contains an entire compressed dependency graph of
changes - often called the "document" format; one which contains a single
change, and one which contains deflate compressed data which is itself a
chunk.

=== Chunk Container

[bytefield, target="chunk-container"]
....
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(def row-height 120)
(draw-column-headers)
(draw-box "magic" {:span 4})
(draw-box "checksum" {:span 4})
(draw-box (text "block type" :vertical))
(draw-box (text "chunk length" :vertical) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-gap "chunk contents")
(draw-bottom)
....

|===
| Field                   | Byte Length     | Description                                          |

| Magic Bytes             | 4               | Some magic bytes, specifically the
sequence `[0x85, 0x6f, 0x4a, 0x83]`|
| Checksum                | 4               | First 4 bytes of the SHA256 of the encoded chunk     |
| Block Type              | 1               | The type of this chunk|
| Chunk length            | Variable (uLEB) | The length of the following chunk bytes              |
| Chunk | Variable        | The actual bytes for the chunk                       |
|===

=== Chunk types
A chunk type is either:

|===
| Value | Description|

| `0` | A document chunk, containing an entire change graph |
| `1` | A change chunk, containing some change metadata and some operations |
| `2` | A deflate compressed chunk |
|===

=== Document Chunks

In order to compress well we encode actor IDs at the start of the document and
operation IDs in the operation just refer to an offset into this list. We also
don't encode the hashes of all the changes, instead we just store the heads of
the graph and we reconstruct the changes and hash them as we decompress the
document.

We encode both change metadata and operations in column oriented fashion. For
each data type we first encode the column metadata followed by the column data.

[bytefield, target="document-chunk-header"]
....
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(def box-width 110)
(def boxes-per-row 8)
(draw-box (text "actors length" ) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-box (text "actors" ) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-box (text "heads length" ) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-box (text "heads" ) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-gap "changes metadata")
(draw-gap "operations metadata")
(draw-gap "change bytes")
(draw-gap "operations bytes")
(draw-bottom)
....


|===
| Field                                       | Byte Length     | Description                                       

| Actors length                               | Variable (uLEB) | The number of following actors                    
| Actors                                      | Variable        | The actor IDs in sorted order                     
| Heads length                                | Variable (uLEB) | The number of following heads hashes              
| Heads                                       | 32 * heads length    | The head hashes of the hash graph in sorted order 
| Changes column metadata                     | Variable        | The change columns metadata                    
| Operations column metadata                  | Variable        | The operations columns metadata
| Change bytes                                | Variable        | The actual bytes for the changes                  
| Operations bytes                            | Variable        | The actual bytes for the operations               
|===

Actor IDs are encoded as an uLEB int length, followed by the corresponding
number of bytes.

==== Changes

Changes are encoded in causal order (a topological sort of the hash graph).

The change metadata contains the column ids that are present in the encoding.
Empty columns (those with no data) are not included.

The possible column IDs are as follows:

|===
| ID  | Name       | Encoding   | Type of Data                                                    
                                                                                                  
| 1   | Actor      | uLEB RLE   | Position of the actor in the sorted actors list                 
| 3   | Seq        | Delta      | Value of the sequence counter for this change                   
| 19  | Max Op     | Delta      | The maximum sequence number of the operations in this change    
| 35  | Time       | Delta      | The timestamp this change was produced at                       
| 53  | Message    | String RLE | The message this change came with                               
| 64  | Deps num   | uLEB RLE   | The number of dependencies this change has                      
| 67  | Deps index | Delta      | The indices of the dependencies, as they appear in the document 
| 86  | Extra len  | uLEB RLE   | Length of the extra bytes                                       
| 87  | Extra raw  | None       | The raw extra bytes                                             
|===


==== Operations

Operations are extracted from changes and grouped by the object that they manipulate.
Objects are then sorted by their IDs to make them appear in causal order too.

The operations informatino contains the column ids that are present in the encoding.
Empty columns (those with no data) are not included.

For each included column the following is encoded:

For each operation we encode its information in the following columns:

|===
| Column            | Type of Data                                                     

| OpID Actor        | Position of the actor part of the OpID in the sorted actor list  
| OpID Counter      | The counter part of this OpID                                    
| Insert            | Whether this operation is an insert or not                       
| Action            | Action type that this operation performs                         
| Object ID actor   | The actor part of the object this operation manipulates          
| Object ID counter | The counter part of the object this operation manipulates        
| Key actor         | The actor part of this key (if a sequence index)                 
| Key counter       | The counter part of this key (if a sequence index)               
| Key string        | The string part of this key (if a map key)                       
| Value ref counter | The counter part of the OpID this cursor refers to (cursor only) 
| Value ref actor   | The actor part of the OpID this cursor refers to (cursor only)   
| Value length      | The length of the encoded raw value in bytes                     
| Value raw         | The actual value                                                 
| Successors number | The number of successors in this operation                       
| Successor actor   | The actor part of the successor                                  
| Successor counter | The counter part of the successor                                
|===

==== Order of operations

Operations must appear in a specific order, as follows:

* First sort by objectId, such that any operations for the same object are consecutive in the file.
  The null objectId (i.e. the root object) is sorted before all non-null objectIds.
  Non-null objectIds are sorted by Lamport timestamp ordering.
* Next, if the object is a map, sort the operations within that object lexicographically by key,
  so that all operations for the same key are consecutive. This sort order should be based on the
  UTF-8 byte sequence of the key. NOTE: the JavaScript implementation currently does not do this
  sorting correctly, since it sorts by JavaScript string comparison, which differs from UTF-8
  lexicographic ordering for characters beyond the basic multilingual plane.
* If the object is a list or text, sort the operations within that object by the position at which
  they occur in the sequence, so that all operations that relate to the same list element are
  consecutive. Tombstones are treated just like any other list element. To determine the list element
  that an operation relates to, the following rule applies: for insertions (operations where the
  insert column is true), the opId is the list element ID; for updates or deletes (where insert is
  false), the key (keyCtr and keyActor columns, known as elemId in the JSON representation) is the
  list element ID.
* Among the operations for the same key (for maps) or the same list element (for lists/text), sort
  the operations by their opId, using Lamport timestamp ordering. For list elements, note that the
  operation that inserted the operation will always have an opId that is lower than the opId of any
  operations that updates or deletes that list element, and therefore the insertion operation will
  always be the first operation for a given list element.

==== Hash verification

TODO: specify how to reconstruct change hashes from the document and verify that the heads match

==== Encoding algorithm

TODO: write down the algorithm for encoding a document based on the above

==== Decoding algorithm

TODO: write down the algorithm for decoding a document based on the above


=== Change chunks

TODO

=== Compressed chunks

TODO


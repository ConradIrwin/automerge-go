= Binary Document Format
Alex Good <alex@memoryandthought.me>; Andrew Jeffery <andrewjeffery97@gmail.com>
:descriptions: A specification of the automerge storage format
:revremark: draft
:toc:
:toclevels: 4
:stylesheet: asciidoctor.css

== Introduction

Automerge documents are a directed acyclic graph (DAG) of changes. Each change
consists of an actor ID, a list of parent changes, and a list of operations.
We refer to these conceptual structures as the "reference document", "reference
change", and "reference operation" and collectively as the "reference data
model". This specification describes a compressed storage format which can be
mapped to the reference data model.

== Terminology and Conventions

The key words "`MUST`", "`MUST NOT`", "`REQUIRED`", "`SHALL`", "`SHALL NOT`",
"`SHOULD`", "`SHOULD NOT`", "`RECOMMENDED`", "`NOT RECOMMENDED`", "`MAY`", and
"`OPTIONAL`" in this document are to be interpreted as described in <<RFC2119>>
and <<RFC8174>> when, and only when, they appear in all capitals, as
shown here.


== Concepts

=== Actor IDs

Actors represent sequential threads of operation in automerge. An actor ID is an
arbitrary sequence of bytes.

=== Operations 

Operations are individual mutations of a document which are bundled together in
changes. Some operations carry a payload.

=== Operation IDs

Every operation is identified by the pair (`actor ID`, `counter`). The actor ID
is the ID of the actor who created the operation, counter is an integer which
always increases for each actor.

[#objects-intro]
=== Objects and object IDs

Some data types in an automerge document are composite objects, either maps from
strings to values - the `map` and `table` objects - or sequences - `list` and
`text`. Operations on these objects reference objects by the operation ID of the
operation which creates the object. The null object ID indicates that the
operation target is the "root" object, which is a map.

=== Dependency Graph

Changes in automerge form a DAG expressed by a list of `SHA256` hashes of direct
ancestors stored in the `dependencies` field of the <<change-reference,
change>>.

[#lamport-timestamp]
=== Lamport timestamp ordering

Operation IDs are lamport timestamps. This imposes a total ordering. To compare
two lamport timestamps:

* If the counter components are different then whichever timestamp has the
  larger counter is the larger
* If the counter components are the same but the actor IDs are different then
  the actor ID which is lexicographically larger is considered the larger
  timestamp
* Otherwise the two timestamps are equal


=== Reference Data Model

The reference data model is an abstract representation of an automerge
document. Implementations will generally choose application specific
representations of the data types but the encoding is defined in terms of
mappings to and from the reference data model.

Note that for forwards compatibility the reference data model may contain fields
which we don't know the name or purpose of. This data may be produced by future
versions of automerge and MUST be retained when mapping to and from the
reference data model. See <<unknown-columns, unknown colums>> for details on how
this is achieved in the columnar storage format.


[#change-reference]
==== Change

|===
| Field | Type | Description

| Actor ID | Arbitrary byte sequence | Unique actor ID
| Seq | 64 bit Integer | Sequence number, always increasing per-actor
| Message | Optional byte sequence | Human readable message describing this
change
| Dependencies | List of 32 byte arrays | List of hashes of parent changes
| Operations | List of operations | The operations in this change
| Extra bytes | Arbitrary byte sequence | Extra data reserved for forward
compatibility reasons
3+| ... other unknown fields ... |
|===

==== Operations

|===
| Field | Type | Description

| Object ID | Operation ID | The ID of the object this operation pertains to
| Key | String or operation ID | The map property or sequence element within the
object
| Action | Action | The change this operation is making
| Value | Optional <<primitive-values, primitive value>> | The payload of this operation (if any)
| Pred | List of operation IDs | Previous operations this operation supercedes
| Unknown field 1 | an <<unknown-field-values, unknown field>>| forward
compatible data
3+| ... other unknown fields ... |
|===

The action of an operation can be one of a few different types:

`makeMap`, `makeTable`, `makeList`, `makeText` :: Operations which denote
creation of a new composite object. The ID of the operation becomes the ID of
the resulting object as noted in <<objects-intro,objects>>.
`del` :: Marks the key within the object as deleted
`inc` :: Increments the counter stored at the given object and key
`set` :: Set the value at the given object and key

The `inc` and `set` operations have an associated `value` field which is a
<<primitive-values, primitive value>>. For all other operations `value` is `null`.

[#primitive-values]
==== Primitive Values

Primitive values can be any of the following

|===
| Type | Description

| bytes | Arbitrary sequnce of bytes 
| string | A valid UTF-8 string
| int | 64 bit integer
| float | 64 bit floating point number
| counter | 64 bit positive integer
| timestamp | 64 bit positive integer
| boolean | boolean
| null | the null value
|===

Technically the `counter` and `timestamp` types are not primitive but they are
still treated separately in the data model.

[#unknown-field-values]
==== Unknown field values

Unknown fields may contain either a <<primitive-values, primitive value>> or a
list of lists of primitive values.

== Columnar Storage Format

=== Overview

This section specifies a general storage format. This format is used to encode
several different kinds of data within the different <<chunk-containers, chunk
types>> of an automerge document. Notably, this storage format is designed to be
forward compatible, see the section on <<unknown-columns, unknown columns>>.

=== Simple types

==== uLEB and LEB

uLEB is an unsigned https://en.wikipedia.org/wiki/LEB128[little endian base 128] value.
This is a variable length encoding used throughout this document.

LEB is the signed variant.

[#action-array]
==== Action array

The actions of the reference data model are encoded in the storage format as a
0-based index into the following array:

|===
| Action

| `makeMap`
| `set`
| `makeList`
| `del`
| `makeText`
| `inc`
| `makeTable`
| `link`
|===

WARNING: Link is unusued I think?


[#column-metadata-block]
=== Column Metadata

Data stored in columnar format is made up of two parts, a metadata block and a
data block. The metadata block is length delimited:

|===
| Field | Description

| Num columns | uLEB of the number of columns in the metadata
| Column metadata | The bytes containing the  metadata
|===

The column metadata consists of pairs of the form

|===
| Field | Description

| <<column-specifications, Column Specification>> | a uLEB integer
| Column data length | uLEB encoding of the length of the data for this column in the data
block 
|===

The data for each column is in the data block in the same position as the
respective column occurs in the metadata block. The column specification encodes
how to interpret the data in the data block.

==== Order of columns

Columns MUST be encoded in ascending <<normalized-column-specification, normalized
column specification>> order, implementations MUST abort parsing a document if
the columns are not in this order.

The column data is encoded in the data block in the same order as the column
metadata.

[#column-specifications]
==== Column Specifications

Column specifications are a uLEB encoded integer which should be interpreted
as a bitfield like so:

WARNING: This allows column IDs to be arbitrarily large which means
implementations will need to choose how large they want to allow them to be
(e.g. the javascript implementation choose 2^53 whilst the rust implementation
uses 32 bit integers. This may be a problem because column IDs are not actually
integers so there's no reason to think that they won't hit the ends of these
ranges. We shold probably specify a size.

[bytefield,target="column-id-layout"]
....
(def boxes-per-row 32)
(def row-height 100)
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(draw-column-headers {:labels (map str (reverse (take 32 (iterate inc 1))))})
(draw-box "ID" {:span 28})
(draw-box (text "DEFLATE" :vertical) {:span 1})
(draw-box "type" {:span 3})
....

* The least significant three bits encode the column type
* The 4th least significant bit is `1` if the column is <<DEFLATE>> compressed and
  `0` otherwise
* The remaining bits are the column ID

Implementations MUST abort if duplicate column specifications are detected when
parsing.

If the deflate bit is set then the column data must first be decompressed using
DEFLATE before proceeding with decoding the values.

The column type specifies how the data in the column is encoded. The possible
types are:

[#column-types-table]
|===
| Value | Description | Encoding

| 0 | <<group-columns,Group>> | RLE compressed uLEB
| 1 | <<actor-index-columns, Actor Index>> | RLE compressed integer
| 2 | Integers | RLE compressed LEB
| 3 | Positive integers | Delta compressed uLEB
| 4 | Booleans | Boolean
| 5 | Strings | RLE compressed utf-8
| 6 | <<raw-value-columns, Raw value metadata>> | RLE compressed LEB
| 7 | <<raw-value-columns, Raw values>> | Raw values
|===

[#normalized-column-specification]
===== Normalized column specification

Because columns can be optionally compressed there are two possible encodings of
the same column specification - one with and one without the compression bit set.
Column specifications are normalized by setting their 4th least significant bit
to 0.

[#column-encodings]
=== Column Encodings

All columns MUST have the same number of values. Note that for grouped columns
this refers to the number of values in the group column and for value columns
this refers to the number of values in the value metadata column. The only
exception to this rule is if every single value in the column is `null` (for
column types which admit a null value), in which case the column length may be
`0` and implementations must fill in null values for each row.

[#actor-index-columns]
==== Actor Index

Columns which contain actor IDs. Actor IDs are repeated frequently so the column
value is an index into an array of actors encoded elsewhere. The exact nature of
the mapping from the index to an actor ID depends on the <<chunk-containers,
chunk type>>.

[#group-columns]
==== Group

A group column specifies a composite, collection-valued column. Column
specifications following the group column specification in the metadata block
which have the same ID as the group column specification should be read
together, these are the "grouped columns". The group column data consists of
<<rle-columns, run length encoded integers>>, the value for each row determines
how many values should be read from each of the grouped columns. Implementations
MUST abort if they cannot read this number of values from each of the grouped
columns.

An example of this is the `pred` column in the change encoding. The portion of
the metadata block containing the pred column specification is encoded thusly

[svgbob, target="group-example"]
....
.-----+------------+-----+------------+-----+-----------.
| 112 | <data len> | 113 | <data len> | 115 | <data len>|
| ...                                                   |
`-------------------------------------------------------'
....

* `112` is `(7 << 4)`, thus the type is `0` which means this is a group column.
  With ID `7`
* `113` is `(7 << 4) | 1` so the type is `1` which is "actor" and the column
  id is `7`
* `115` is `(7 << 4) | 3` so the type is `3` which is "delta int" and the column
  ID is `7`

To read values from this column then we first decode the value of the group
column, then we decode this number of values from each of the grouped columns
and the value for the row becomes the list of lists of resulting values. In this
case if we read `n` from the group column then the row value would be `[[actor1,
counter1], [actor2, counter2], ..., [actor_n, counter_n]]`

Note that it is not possible for two columns in a group to have the same type as
it would not be possible to have a deterministic ordering for the column
specifications. Implementations MUST abort if they encounter two column
specifications with the same type and column ID.

Implementations MUST abort if they encounter multiple group column
specifications with the same ID.

Group column specifications must be followed by at least one column
specification with the same column ID. Implementations MUST abort if a group
column specification without a following column specification of the same ID is
encountered.


[#rle-columns]
==== RLE

Run length encoding of values. The type of values in a column will be one of
the following: 

* uLEB encoded integers
* LEB encoded integers
* Length prefixed UTF-8 strings which consist of a uLEB encoded integer
  followed by that number of UTF-8 bytes

For a given RLE column the type is specified by the column type in
<<column-types-table, column types>>. 

A "run" in an RLE columns is encoded as pairs of the form `(length,value)`.
`length` is a signed LEB encoding of the length of the run. the interpretation
of `value` depends on `length`.

* If `length` is positive, then `value` is a single instance of the value which
  occurs `length` times.
* If `length` is 0 then this pair represents a `null` value and `value` is the
  uLEB encoding of the number of times `null` occurs
* If `length` is negative then `value` is a literal run and the absolute value
  of `length` is the number of items in the literal run. That is to say, there
  is no compression.

==== Delta

This encoding is only applicable for columns which contain positive integer
datatypes. The encoded data is a sequence of uLEB integers. The value starts as
`0` and each new item is encoded as the difference between the new value and the
current value. This sequence of deltas is then run length encoded as per the run
length encoding section.

For example, the sequence 

....
[1,2,3,4,5,10,15]
....

Would be encoded as 

....
[1,1,1,1,1,5,5]
....

This sequence is then run length encoded to given

....
[(5,1), (2,5)]
....

WARNING: What should the null value be for a delta column? How should
applications handle a decoded delta value which takes the absolute value below
zero?

==== Boolean

This encoding is only available for columns containing booleans. The column
contains sequences of uLEB integers which represent alternating sequences of
`false/true`. The initial value of the column is always `false`

For example, the sequence `[0,2,3]` would be `[true, true, false, false,
false]`.


[#raw-value-columns]
==== Raw values

Raw value fields are encoded as two column specifications. The first has type
`6`, indicating that it is raw value metadata and the second has type `7`,
indicating that it contains raw values. The two columns have the same ID. 

Note that raw value columns which do not contain values may be omitted. If
implementations encounter a lone value metadata column they must assume that it
is accompanied by an empty raw value column.

Implementations must abort if they encounter  a raw value column not preceeded
by a metadata column with the same id. implementations must also abort if they
encounter more than one metadata column with the same column id, or more than
one raw value column with the same id.

These two colums are intepreted together. The metadata column contains RLE
compressed LEB integers. These integers are laid out like so

[bytefield,target="raw-value-metadata-layout"]
....
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(draw-column-headers {:labels (reverse column-labels)})
(draw-box "length" {:span 13 :borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-box "type" {:span 2})
....

* The lower four bits encode the type of the value
* The higher bits encode the length of the value

The type code may be 

|===
| Value | Type 

| 0 | Null
| 1 | False
| 2 | True
| 3 | uLEB 
| 4 | LEB
| 5 | IEEE754 float
| 6 | UTF8 bytes
| 7 | Bytes
| 8 | Counter
| 9 | Timestamp
|===

If the type tag is none of these values it may be a value produced by a future
version of automerge. In this case implementations MUST read and store the type
code and raw bytes when reading and write them back in same position when
writing.

The interpretation of the value column depends on the type code. 

* For `0,1,2` (`null`, `false`, `true`) no value is stored in the raw value
  column
* For all other column types the length bits specify the number of bits which
  should be read from the raw value column (which is not compressed in any
  manner) and interpreted as follows:
** `uLEB` and `LEB` as per the LEB128 spec
** IEEE754 floats - as per the spec
** UTF8 bytes should be interpreted as a string. Implementations SHOULD validate
   that the bytes are valid UTF8 and replace any offending characters with
   U+FFFD REPLACEMENT CHARACTER
** Bytes - the data is an arbitrary byte sequence
** Counter, the underlying data is a uLEB encoded integer.
** Timestamp, the underlying data is a uLEB encoded integer.

WARNING: Replacing invalid utf-8 seems like it might be a bad idea? Should check
this. I _think_ it's what the javascript implementation does though.

[#unknown-columns]
=== Unknown columns

When reading the column metadata applications may encounter column
specifications which they are not expecting. These column specifications may be
produced by future versions of the application. If an implementation encounters
an unknown column whilst reading data it MUST retain this data when writing that
data back to storage.

This is possible because every column type has some concept of a null value.
When inserting new rows into a collection of rows stored in the columnar storage
format application MUST write a null value into columns which they do not
recognise for the new rows they are inserting.

WARNING: What should the null value be for boolean columns?

== File structure

An automerge file consists of one or more length delimited chunks.
Implementations must attempt to read chunks until the end of the file. There are
three types of chunk, one which contains an entire compressed dependency graph of
changes - often called the "document" format; one which contains a single
change, and one which contains deflate compressed data which is itself a
chunk.

[#chunk-containers]
=== Chunk Container

[bytefield, target="chunk-container"]
....
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(def row-height 120)
(draw-column-headers)
(draw-box "magic" {:span 4})
(draw-box "checksum" {:span 4})
(draw-box (text "block type" :vertical))
(draw-box (text "chunk length" :vertical) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-gap "chunk contents")
(draw-bottom)
....

|===
| Field                   | Byte Length     | Description

| Magic Bytes             | 4               | Some magic bytes, specifically the
sequence `[0x85, 0x6f, 0x4a, 0x83]`
| Checksum                | 4               | First 4 bytes of the SHA256 of the encoded chunk
| Block Type              | 1               | The type of this chunk
| Chunk length            | Variable (uLEB) | The length of the following chunk bytes
| Chunk | Variable        | The actual bytes for the chunk
|===

If the first four bytes are not exactly the magic bytes implementations MUST abort.

[#hash-calculation]
==== Hash and checksum calculation

The hash is the <<SHA256>> hash of the concatenation of the chunk length
and chunk contents fields. The checksum calculated from this hash is the first
four bytes of the hash. Implementations MUST abort reading if the checksum does
not match.


=== Chunk types
A chunk type is either:

|===
| Value | Description

| `0` | A <<document-chunks, document chunk>>, containing an entire change graph
| `1` | A <<change-chunks, change chunk>>, containing some change metadata and some operations
| `2` | A deflate <<compressed-chunks, compressed chunk>>
|===

[#document-chunks]
=== Document Chunks

Document are stored in the following manner:

[bytefield, target="document-chunk-header"]
....
(defattrs :vertical [:plain {:writing-mode "vertical-rl"}])
(def box-width 110)
(def boxes-per-row 8)
(draw-box (text "actors length" ) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-box (text "actors" ) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-box (text "heads length" ) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-box (text "heads" ) {:borders #{:left :top :bottom}})
(draw-gap-inline)
(draw-gap "changes metadata")
(draw-gap "operations metadata")
(draw-gap "change bytes")
(draw-gap "operations bytes")
(draw-gap "head indexes")
(draw-bottom)
....


|===
| Field                                       | Type            | Description                                       

| Actors length                               | uLEB | The number of following actors                    
| Actors                                      | Array of actor IDs        | The actor IDs in sorted order                     
| Heads length                                | uLEB | The number of following heads hashes              
| Heads                                       | 32 * heads length long byte
array    | The head hashes of the hash graph in sorted order 
| Changes column metadata                     | <<column-metadata-block, column
metadata>>        | The change columns metadata                    
| Operations column metadata                  | <<column-metadata-block, column
metadata>>| The operations columns metadata
| Change bytes                                | Column data        | The actual bytes for the changes                  
| Operations bytes                            | Column data        | The actual bytes for the operations               
| <<head-indexes,Head indexes>>               | Array of uLEB | The indices of the heads in the changes
|===

Actor IDs are <<length-prefixed-actor-ids,length prefixed>>. Implementations
MUST abort if the actors array is not lexicographically ordered.

A single document contains many changes. Change metadata is encoded separately
to operation data in a column oriented format using the change column metadata
and change bytes above, whilst the operations are encoded using the operations
column metadata and operations bytes. The process of decoding these consists of
first reading all the operation data, then the change metadata using the
procedures outlined in <<column-encodings, column encodings>>, then matching up
operations with their change metadata to construct the reference document.

[#document-actor-lookup]
==== Actor lookup

Actors in the document encoding are encoded in lexicographic order in the actors
array at the start of the document. Actor indexes throughout the document refer
to the index into this array. We use the syntax `lookup_actor(actor_index)` to
refer to this procedure.


[#document-operations]
==== Operations

The columns in the operation storage are at least the following:

|===
| Field | Specification | Type | Description
 
| Object actor | 1 | Actor index | actor index of object ID this operation targets
| Object counter | 2 | RLE compressed uLEB | counter of the object ID this operation targets
| Key actor | 17 | Actor index | actor of the operation ID of the key of this operation
| Key counter | 19 | Delta compressed uLEB | counter of the operation ID of the key of this
  operation
| Key string | 21 | RLE compressed utf-8 | The string key this operation targets
| actor | 33 | Actor index | The actor of this operations ID
| counter | 35 | Delta compressed uLEB | The counter of this operations ID
| insert | 52 | Boolean | Whether or not this is an insert operation
| action | 66 | RLE compressed uLEB | The action index of this operation
| value metadata | 86 | Value metadata | The metadata for the value of this operation
| value | 87 | Value contents | The value of this operation
| successor group | 128 | Group | The group for the successors of this operation
| successor actor | 129 | Actor index | The actor of each successor operation ID of this operation
| successor counter | 131 | Delta compressed uLEB | The counter of each successor operation ID of
this operation
|===

WARNING: The javascript implementation includes a `child` column, is this
required?

Any unknown columns MUST be preserved when decoding and written back out when
encoding as per <<unknown-columns, unknown columns>>.

We determine the key that the operation refers to thusly:

* If the key string is not null then this is the key of the operation
* Otherwise we use the pair (lookup_actor(key actor), key counter) as the key of the operation
* If key string is null and any of key actor or key counter are null
  implementations MUST abort

Using this procedure we can write the operations as:

|===
| Field | Type | Mapping to columns

| Object | Operation ID | (lookup_actor(object actor), object counter)
| Key | either string or operation ID | The value determined above
| Id | Operation ID | (lookup_actor(actor), counter)
| Insert | boolean | insert
| Action | action | <<action-array, action index lookup>>
| Value | primitive value | value metadata and value columns
| Successors | list of operation ID | (lookup_actor(actor), counter) for actor,
and counter in the success group column
|===

==== Order of operations

Operations are grouped by the object that they manipulate. Objects are then
sorted by their IDs. Thus operations are ordered using the following procedure:

WARNING: Is this required? If so should implementations abort if the operations
are not inthis order?

* First sort by object ID, such that any operations for the same object are
  consecutive. The null objectId (i.e. the root object) is sorted before all
  non-null objectIds. Non-null objectIds are sorted by <<lamport-timestamp,
  Lamport timestamp>>.
* For each object:
** if the object is a map, sort the operations within that object
   lexicographically by key, so that all operations for the same key are
   consecutive. This sort order MUST be based on the UTF-8 byte sequence of the
   key. 
** If the object is a list or text, sort the operations within that object by the
   operation ID of the element they target. This is determined as follows:
*** For insert operations the target element is the operation ID of the
    inserting operation
*** For `set` or `delete` operations the target is the operation ID in the `key`
    field
* Among the operations for the same key (for maps) or the same list element (for
  lists/text), sort the operations by their opId, using <<lamport-timestamp,
  lamport timestamp>> ordering. For list elements, note that the operation that
  inserted the operation will always have an opId that is lower than the opId of
  any operations that updates or deletes that list element, and therefore the
  insertion operation will always be the first operation for a given list
  element.


WARNING: the JavaScript implementation currently does not do this sorting
correctly, since it sorts keys by JavaScript string comparison, which differs
from UTF-8 lexicographic ordering for characters beyond the basic multilingual
plane.

===== Successors and omitting deletes

The document storage format does not encode a predecessors field. Instead this
information is encoded in the `successors` field. This can be used to
reconstruct the predecessors field from the reference data model.

Delete operations do not carry any information other than the object ID and key
they are deleting. As such they are encoded in the document by appending the
operation ID of the delete operation to the successors of the operation creating
the data to be deleted.

Implementations MUST abort if they encounter explicitly encoded delete
operations in a document chunk.

[#document-change-metadata]
==== Change Metadata

The columns in the change metadata are at least the following:

|===
| Name | Specification | Type | Description

| Change actor | 1 | Actor  |
| Sequence number | 3 | Delta compressed uLEB |
| maxOp | 19 | Delta compressed uLEB | The largest counter that occurs in this
change
| time | 35 | Delta compressed uLEB |
| message | 53 | RLE Compressed UTF-8 |
| dependencies group | 64 | Group |
| dependencies index | 67 | Delta compressed uLEB |
| value metadata | 86 | Value metadata |
| value | 87 | Value raw |
|===

Any unknown columns MUST be preserved when decoding and written back out when
encoding as per <<unknown-columns, unknown columns>>.

Each row in the column oriented change metadata therfore can be written as:

|===
| Field | Type | Mapping

| Actor | positive integer | lookup_actor(change actor)
| Seq | positive integer | sequence number
| maxOp | positive integer | maxOp
| time | positive integer | time
| message | utf-8 | message
| deps | list of integers | read dependencies group and dependencies
index columns
| extra | primitive value | read the value metadata and value raw columns
|===

The `deps` field refers to the index of the changes this change depends on in
the change metadata rows. Implementations MUST abort if `deps` references an
index which is out of bounds.

For a given actor the `seq` field of changes must strictly increase by `1`.
Implementations MUST abort if there are missing changes for a given actor ID.

The `maxOp` field of the change refers to the largest counter component of an
operation ID in the set of operations in this change. For a given actor ID this
must always increase. Implementations MUST abort if the `maxOp` of a change is
not larger than all the `maxOp` of changes from that actor with smaller `seq`.

==== Mapping to the reference data model

Operations in the document format are not stored in the order they were
generated, as they are in the change data model. Furthermore, oeprations in the
document format have a `successor` rather than `predecessor` field. The
following procedure specifies how to map from document operations to the change
operations. "document operation" refers to the data structure derived at the end
of <<document-operations, document operations>> and "document change" refers to
the data structure dervied at the end of <<document-change-metadata>>.

First expand operations:

* Add an empty predecessor list to every document operation
* For each operation in the document operation rows
** For each operation ID in the successors list of the document operation lookup
   the target operation in the document operations:
*** If an operation is found add the current operation ID to the
    target operations predecessor list
*** If no operation is found then insert a new delete operation into the
    document with its ID set to the target operation ID, the object and key
    set to the same value as the current operation, and the predecessor set to
    the current operation.

Second, match up changes:

For each document operation

* Sort all the changes for the same actor as the operation ID by ascending
  `maxOp`
* Add the document operation to the first change which has `maxOp >= counter`
  where `counter` is the counter component of the operation ID.

Implementations MUST abort if no matching change is found

For each change sort the operations within the change by
<<lamport-timestamp>> of the operation ID.

===== Hash verification

The dependencies in the document model are expressed as integer offsets. But in
the reference data model dependencies are expressed as a hash of the ancestor
changes. To map to the hash based representation perform a topological traversal
of the dependency graph and for each change encode the change as per
<<change-chunks>> and then calculate the hash of the change as in
<<hash-calculation>>, then for every change replace the index of the current
change with the calculated hash.

Once this procedure is complete take the heads of the depedency graph and
compare their hashes with the head hashes field in the document chunk. If the
hashes don't match implementations MUST abort.

[#head-indexes]
==== Head Indexes

The head indexes is an array of uLEBs. There is one index for each head, the
indexes are in the same order as the head hashes. Each index refers to the
change in the change metadata to which the head hash is a reference.

[#change-chunks]
=== Change chunks

The fields in a change chunk, in order, are:

|===
| Field | Type | Description

| Dependency count | uLEB | The number of hashes in the dependencies fields
| Dependencies | 32 * dependency count long byte array | The dependency hashes
| Actor length | uLEB | The length of the actor ID
| Actor | byte array | The actor ID
| Sequence number | uLEB | The sequence number
| Start op | uLEB | The counter of the first op in this change 
| Time | uLEB | The time this change was created in milliseconds since the unix
epoch
| Message length | uLEB | The length of the message
| Message | UTF-8 | The message associated with this change
| Other actors length | uLEB | The number of other actor IDs in this change
| Other actors | byte array | The other actor IDs
| ops column metadata | <<column-metadata-block, Ops column metadata>> | The
metadata for the column oriented operation encoding 
| Ops column data | ops column data | The column data for the operations
| Extra bytes | Byte array | Any data remaining in the chunk
|===

Each actor ID in the other actors array is a <<length-prefixed-actor-ids, length
prefixed actor ID>>. 

The actor IDs in the other actors array are lexicographically ordered.
Implementations MUST abort when parsing a change which does not present the
actors in this order.

Note the extra data must be retained when writing the change back to storage.

[#change-operation-columns]
==== Change operation columns

The column specifications in the operation metadata must include the following
(note that the column types are redundant as they are included in the
specification but we elaborate them for clarity):


|===
| Field | Specification | Type | Description

| object actor |1   | Actor Index | The actor of the ops object ID
| object counter  |2   | RLE compressed uLEB | The counter of the ops object ID
| key actor |17  | Actor | The (optional) actor of the ops key 
| key counter |19  | Delta Compressed uLEB | The (optional) counter of the ops key 
| key string |21  | RLE Compressed UTF-8 | The (optional) string of the ops key
| ID actor |33  | Actor index | The actor of the ops op ID
| ID counter |35  | Delta compressed uLEB | The counter of the ops op ID
| insert |52  | Boolean | Whether or not this is an insert operation
| action index |66  | RLE compressed uLEB | The <<action-array, action index>> for the op
| value metadata |86  | Value meta | The value metadata for the op
| value raw |87  | Value raw | The raw value for the op
| pred group |112 | Group | The <<group-columns, group column>> for the
predecessors of this op
| pred actor index |113 | Actor | The actor component of the predecessors
| pred counter |115 | RLE Compressed uLEB | The counter component of the predecessors
|===

WARNING: The javascript implementation includes a `child` actor ID here. It
doesn't seem to be needed though, is it obsolete?

Reading implementations MUST abort if any of these column specifications are not
present.

There may be additional columns present, implementations MUST read these columns
when translating to the reference data model as per <<unknown-columns, unknown
columns>>.

===== Compressed columns

Compressed columns are not permitted in change chunks. Implementations MUST
abort if they encounter a column specification with the deflate bit set.

[#change-actor-lookup]
==== Actor ID lookup

All actor columns resolve to integers. These integers are offsets into the
concatenation `[change actor ID] + other actor IDs` from the change metadata.
Implementations MUST abort if an actor index is read which is not present in
this concatenation.

==== Mapping to the reference data model

We lookup actors using the notation `lookup_actor(actor Index)` which refers to
the process specified in <<change-actor-lookup, actor ID lookup>>.

We determine the key that a row refers to thusly:

* If the key string column is not null then this is the key of the operation
* Otherwise we use the pair (lookup_actor(key actor), key counter) as the key of the operation
* If key string is null and any of key actor or key counter are null
  implementations MUST abort

We can then map the columns to the reference data model as follows:

|===
| Field | Mapping from column

| Object ID | `(lookup_actor(object actor), object counter)`
| Op ID | `(lookup_actor(ID actor), ID counter)`
| Key | The key determined above
| Action | The action from the <<action-array>> corresponding to the action
index
| Value | The value read from the value metadata and value raw columns
| Pred | `(lookup_actor(pred_actor), pred_counter)` for pred_actor and
pred_counter in the predecessors group column
|===

For each unknown column in the column metadata implementations MUST add the
value of that unknown column to the reference operation. Implementations MUST
store the column specifications as well as the values so that the unknown values
can be written back out when mapping from the reference operation back to the
change chunk.

The reference change then becomes:

|===
| Field | Change metadata field

| Actor ID | lookup_actor(change actor)
| Seq | Change sequence number
| Message | Change message
| Dependencies | Change dependencies
| Operations | The operations mapped above
| Extra bytes | The extra bytes in the change
|===

[#compressed-chunks]
=== Compressed chunks

Compressed chunks must be decompressed using <<DEFLATE>>. The decompressed chunk
is a chunk container which should be interpreted as per <<chunk-containers,
chunk containers>>. Implementations SHOULD raise an error if the contents of a
compressed chunk is another compressed chunk.

[#length-prefixed-actor-ids]
=== Length prefixed actor IDs

Actor IDs are stored in length prefixed form as follows

[svgbob, target="length-prefixed-actor"]
....
.--------------+-------.
| Length: uLEB | Bytes |
`--------------+-------'
....


[bibliography]
== References

* [[[RFC2119]]]: https://datatracker.ietf.org/doc/html/rfc2119
* [[[RFC8174]]]: https://datatracker.ietf.org/doc/html/rfc8174
* [[[DEFLATE]]]: https://datatracker.ietf.org/doc/html/rfc1951
* [[[SHA256]]]: https://datatracker.ietf.org/doc/html/rfc4634
